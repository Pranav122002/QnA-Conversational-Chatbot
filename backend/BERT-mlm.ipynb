{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FInetuning BERT using MLM (Masked language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your dataset (list of sentences)\n",
    "dataset = [\n",
    "    \"The temperature in New York is expected to rise tomorrow.\",\n",
    "    \"Heavy rain and thunderstorms are forecasted in the evening.\",\n",
    "    \"A cold front will move through the region by the weekend.\",\n",
    "    \"Winds gusting up to 30 miles per hour are anticipated.\",\n",
    "    \"The humidity levels will drop significantly next week.\",\n",
    "    \"Lata Ragha is HOD of college.\",\n",
    "    \"The weather conditions might affect outdoor events.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your dataset\n",
    "tokenized_input = tokenizer(dataset, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Mask a percentage of tokens in the dataset\n",
    "# For instance, mask 15% of tokens with '[MASK]' token\n",
    "masked_input = tokenized_input.input_ids.clone()\n",
    "mask_indices = torch.bernoulli(torch.full(masked_input.shape, 0.15)).bool() & (masked_input != tokenizer.pad_token_id)\n",
    "masked_input[mask_indices] = tokenizer.mask_token_id\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 5.781792163848877\n",
      "Epoch 2/3 - Loss: 5.332117080688477\n",
      "Epoch 3/3 - Loss: 4.949427604675293\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on the masked language modeling task\n",
    "for epoch in range(3):  # Assuming 3 epochs for demonstration\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(masked_input, labels=masked_input)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}/3 - Loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('path/to/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.43411219120025635, 'token': 2073, 'token_str': 'where', 'sequence': 'where is hod of college?'}, {'score': 0.2793853282928467, 'token': 2040, 'token_str': 'who', 'sequence': 'who is hod of college?'}, {'score': 0.09936191886663437, 'token': 2054, 'token_str': 'what', 'sequence': 'what is hod of college?'}, {'score': 0.0849163681268692, 'token': 2129, 'token_str': 'how', 'sequence': 'how is hod of college?'}, {'score': 0.01218127179890871, 'token': 2002, 'token_str': 'he', 'sequence': 'he is hod of college?'}]\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "model = BertForMaskedLM.from_pretrained('path/to/save')\n",
    "\n",
    "# Use the model for question answering\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example question\n",
    "question = \"[MASK] is HOD of college ?\"\n",
    "\n",
    "\n",
    "# Get answers using the fine-tuned model\n",
    "answers = fill_mask(question)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where\n"
     ]
    }
   ],
   "source": [
    "best_answer = answers[0]['token_str']\n",
    "print(best_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
