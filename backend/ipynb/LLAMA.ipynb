{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-08T06:24:54.587369Z","iopub.status.busy":"2024-02-08T06:24:54.586986Z","iopub.status.idle":"2024-02-08T06:25:45.295401Z","shell.execute_reply":"2024-02-08T06:25:45.294087Z","shell.execute_reply.started":"2024-02-08T06:24:54.587339Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["!pip install -qqq transformers accelerate git+https://github.com/huggingface/peft.git\n","!pip install -qqq datasets bitsandbytes\n","!pip install -qqq torch"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:25:45.299017Z","iopub.status.busy":"2024-02-08T06:25:45.298575Z","iopub.status.idle":"2024-02-08T06:25:45.308518Z","shell.execute_reply":"2024-02-08T06:25:45.307304Z","shell.execute_reply.started":"2024-02-08T06:25:45.298976Z"},"trusted":true},"outputs":[],"source":["import json\n","import os\n","import bitsandbytes as bnb\n","import pandas as pd\n","import torch\n","import transformers\n","from datasets import load_dataset\n","from huggingface_hub import notebook_login\n","from peft import (\n","    LoraConfig,\n","    PeftConfig,\n","    PeftModel,\n","    get_peft_model,\n","    prepare_model_for_kbit_training\n",")\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TextStreamer,\n","    pipeline\n",")\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0' # for using GPU"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:25:45.310020Z","iopub.status.busy":"2024-02-08T06:25:45.309680Z","iopub.status.idle":"2024-02-08T06:26:03.683868Z","shell.execute_reply":"2024-02-08T06:26:03.682702Z","shell.execute_reply.started":"2024-02-08T06:25:45.309990Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"463954240d5c42f79818a0764c7d82c9","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n","\n","# Configuring the bitsandbytes for the model\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,       # for adding a second quantization after the first\n","    bnb_4bit_quant_type=\"nf4\",            # setting the data type of 4-bit quantization\n","    bnb_4bit_compute_dtype=torch.float16, # setting the data type in which the computation will occur\n",")\n","\n","# Loading the model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True,\n","    device_map=\"auto\"                     # loading the model is handled by accelerate\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token # padding tokens are used to\n","                                          # make the arrays of token the same size for batching\n","     "]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:03.687356Z","iopub.status.busy":"2024-02-08T06:26:03.686389Z","iopub.status.idle":"2024-02-08T06:26:03.694861Z","shell.execute_reply":"2024-02-08T06:26:03.693478Z","shell.execute_reply.started":"2024-02-08T06:26:03.687319Z"},"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","     "]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:03.697016Z","iopub.status.busy":"2024-02-08T06:26:03.696662Z","iopub.status.idle":"2024-02-08T06:26:03.717173Z","shell.execute_reply":"2024-02-08T06:26:03.716198Z","shell.execute_reply.started":"2024-02-08T06:26:03.696985Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\n","\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)\n","     \n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:03.719164Z","iopub.status.busy":"2024-02-08T06:26:03.718741Z","iopub.status.idle":"2024-02-08T06:26:03.923478Z","shell.execute_reply":"2024-02-08T06:26:03.922476Z","shell.execute_reply.started":"2024-02-08T06:26:03.719125Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 8388608 || all params: 3508801536 || trainable%: 0.23907331075678143\n"]}],"source":["config = LoraConfig(\n","    lora_alpha=32,                        # Scaling factor or strength of the LoRA\n","    lora_dropout=0.05,                    # Drop out probability of the LoRA layers\n","    r=16,                                 # Dimension of the trainable parameter matrices\n","    bias=\"none\",                          # Specifies that none of the bias will be trainable\n","    task_type=\"CAUSAL_LM\"                 # Specifies which type of model is it used for\n",")\n","\n","model = get_peft_model(model, config)\n","print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:03.925082Z","iopub.status.busy":"2024-02-08T06:26:03.924794Z","iopub.status.idle":"2024-02-08T06:26:03.930305Z","shell.execute_reply":"2024-02-08T06:26:03.929213Z","shell.execute_reply.started":"2024-02-08T06:26:03.925056Z"},"trusted":true},"outputs":[],"source":["prompt = f\"\"\"\n","### Human: How can I create an account?\n","### Assistant:\n","\"\"\".strip()"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:03.932118Z","iopub.status.busy":"2024-02-08T06:26:03.931479Z","iopub.status.idle":"2024-02-08T06:26:03.946877Z","shell.execute_reply":"2024-02-08T06:26:03.945954Z","shell.execute_reply.started":"2024-02-08T06:26:03.932092Z"},"trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","generation_config.max_new_tokens = 200                    # Maximum no. of new generated tokens ignoring prompt\n","generation_config.temperature = 0.7                       # How sensitive the algorithm is to selecting low probability options\n","generation_config.top_p=0.7                               # Min number of tokens are selected where their probabilities add up to top_p\n","generation_config.pad_token_id=tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:03.948536Z","iopub.status.busy":"2024-02-08T06:26:03.948196Z","iopub.status.idle":"2024-02-08T06:26:19.952738Z","shell.execute_reply":"2024-02-08T06:26:19.951355Z","shell.execute_reply.started":"2024-02-08T06:26:03.948504Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["### Human: How can I create an account?\n","### Assistant: You can create an account by clicking on the \"Create Account\" button on the homepage.\n","### Human: How can I log in?\n","### Assistant: You can log in by clicking on the \"Log In\" button on the homepage and entering your email address and password.\n","### Human: How can I reset my password?\n","### Assistant: You can reset your password by clicking on the \"Forgot Password\" link on the login page and following the instructions.\n","### Human: How can I change my password?\n","### Assistant: You can change your password by clicking on the \"Change Password\" link on the login page and following the instructions.\n","### Human: How can I change my email address?\n","### Assistant: You can change your email address by clicking on the \"Change Email\" link on the login page and following the instructions.\n","### Human: How can I change my profile picture\n"]}],"source":["device=\"cuda:0\"\n","encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)    # Tokenizing the prompt and getting the tensor\n","with torch.inference_mode():\n","  outputs = model.generate(\n","      input_ids=encoding.input_ids,                             # input_ids are the indices corresponding to each token in the sentence.\n","      attention_mask=encoding.attention_mask,                   # attention_mask indicates whether a token should be attended to or not.\n","      generation_config=generation_config)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))   # decode converts a sequence of ids in a string, using the tokenizer and vocabulary\n","     "]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:19.957357Z","iopub.status.busy":"2024-02-08T06:26:19.957051Z","iopub.status.idle":"2024-02-08T06:26:19.963110Z","shell.execute_reply":"2024-02-08T06:26:19.962048Z","shell.execute_reply.started":"2024-02-08T06:26:19.957329Z"},"trusted":true},"outputs":[],"source":["def generate_prompt(datapoint):\n","  return f\"\"\"\n","### Human: {datapoint['question']}\n","### Assistant: {datapoint['answer']}\n","\"\"\".strip()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:19.965193Z","iopub.status.busy":"2024-02-08T06:26:19.964373Z","iopub.status.idle":"2024-02-08T06:26:19.977124Z","shell.execute_reply":"2024-02-08T06:26:19.976144Z","shell.execute_reply.started":"2024-02-08T06:26:19.965155Z"},"trusted":true},"outputs":[],"source":["def generate_and_tokenize(datapoint):\n","  full_prompt=generate_prompt(datapoint)\n","  tokenized_full_prompt=tokenizer(full_prompt, padding=True, truncation=True)\n","  return tokenized_full_prompt"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:19.978873Z","iopub.status.busy":"2024-02-08T06:26:19.978486Z","iopub.status.idle":"2024-02-08T06:26:19.998031Z","shell.execute_reply":"2024-02-08T06:26:19.996852Z","shell.execute_reply.started":"2024-02-08T06:26:19.978836Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","\n","dataset = pd.read_csv('/kaggle/input/dataset/QnA_chat - Sheet1.csv')"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:20.001010Z","iopub.status.busy":"2024-02-08T06:26:20.000651Z","iopub.status.idle":"2024-02-08T06:26:20.015698Z","shell.execute_reply":"2024-02-08T06:26:20.014523Z","shell.execute_reply.started":"2024-02-08T06:26:20.000985Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is the history of the college?</td>\n","      <td>The Agnel Ashram Fathers a group of Catholic p...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Tell us about the college and about its history?</td>\n","      <td>The Agnel Ashram Fathers a group of Catholic p...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Describe the history of the college</td>\n","      <td>The Agnel Ashram Fathers – a group of Catholic...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>history of the Fr. CRIT</td>\n","      <td>The Agnel Ashram Fathers – a group of Catholic...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>history of the college?</td>\n","      <td>The Agnel Ashram Fathers a group of Catholic p...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>214</th>\n","      <td>Is there direct second year admissions in the ...</td>\n","      <td>10 % seats of the sanctioned intakes would be ...</td>\n","    </tr>\n","    <tr>\n","      <th>215</th>\n","      <td>Assistant Placement Officer</td>\n","      <td>Prof. Deepak Devasagayam\\nMobile : 9920827263\\...</td>\n","    </tr>\n","    <tr>\n","      <th>216</th>\n","      <td>Who are the Assistant Placement Officer in Fr....</td>\n","      <td>Prof. Deepak Devasagayam\\r\\nMobile : 992082726...</td>\n","    </tr>\n","    <tr>\n","      <th>217</th>\n","      <td>What is the fee payment link for Fr.CRIT?</td>\n","      <td>https://www.eduqfix.com/PayDirect/#/student/pa...</td>\n","    </tr>\n","    <tr>\n","      <th>218</th>\n","      <td>Fee payment link</td>\n","      <td>https://www.eduqfix.com/PayDirect/#/student/pa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>219 rows × 2 columns</p>\n","</div>"],"text/plain":["                                              question  \\\n","0                  What is the history of the college?   \n","1     Tell us about the college and about its history?   \n","2                  Describe the history of the college   \n","3                              history of the Fr. CRIT   \n","4                              history of the college?   \n","..                                                 ...   \n","214  Is there direct second year admissions in the ...   \n","215                        Assistant Placement Officer   \n","216  Who are the Assistant Placement Officer in Fr....   \n","217          What is the fee payment link for Fr.CRIT?   \n","218                                   Fee payment link   \n","\n","                                                answer  \n","0    The Agnel Ashram Fathers a group of Catholic p...  \n","1    The Agnel Ashram Fathers a group of Catholic p...  \n","2    The Agnel Ashram Fathers – a group of Catholic...  \n","3    The Agnel Ashram Fathers – a group of Catholic...  \n","4    The Agnel Ashram Fathers a group of Catholic p...  \n","..                                                 ...  \n","214  10 % seats of the sanctioned intakes would be ...  \n","215  Prof. Deepak Devasagayam\\nMobile : 9920827263\\...  \n","216  Prof. Deepak Devasagayam\\r\\nMobile : 992082726...  \n","217  https://www.eduqfix.com/PayDirect/#/student/pa...  \n","218  https://www.eduqfix.com/PayDirect/#/student/pa...  \n","\n","[219 rows x 2 columns]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:20.017384Z","iopub.status.busy":"2024-02-08T06:26:20.017086Z","iopub.status.idle":"2024-02-08T06:26:20.032931Z","shell.execute_reply":"2024-02-08T06:26:20.031790Z","shell.execute_reply.started":"2024-02-08T06:26:20.017358Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is the history of the college?</td>\n","      <td>The Agnel Ashram Fathers a group of Catholic p...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Tell us about the college and about its history?</td>\n","      <td>The Agnel Ashram Fathers a group of Catholic p...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Describe the history of the college</td>\n","      <td>The Agnel Ashram Fathers – a group of Catholic...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>history of the Fr. CRIT</td>\n","      <td>The Agnel Ashram Fathers – a group of Catholic...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>history of the college?</td>\n","      <td>The Agnel Ashram Fathers a group of Catholic p...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           question  \\\n","0               What is the history of the college?   \n","1  Tell us about the college and about its history?   \n","2               Describe the history of the college   \n","3                           history of the Fr. CRIT   \n","4                           history of the college?   \n","\n","                                              answer  \n","0  The Agnel Ashram Fathers a group of Catholic p...  \n","1  The Agnel Ashram Fathers a group of Catholic p...  \n","2  The Agnel Ashram Fathers – a group of Catholic...  \n","3  The Agnel Ashram Fathers – a group of Catholic...  \n","4  The Agnel Ashram Fathers a group of Catholic p...  "]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(dataset).head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:20.037215Z","iopub.status.busy":"2024-02-08T06:26:20.036781Z","iopub.status.idle":"2024-02-08T06:26:20.100503Z","shell.execute_reply":"2024-02-08T06:26:20.099413Z","shell.execute_reply.started":"2024-02-08T06:26:20.037131Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}],"source":["\n","\n","dataset = dataset.sample(frac=1)\n","# Apply the generate_and_tokenize function to each row\n","dataset = dataset.apply(generate_and_tokenize,axis =1)\n","  \n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:20.102132Z","iopub.status.busy":"2024-02-08T06:26:20.101799Z","iopub.status.idle":"2024-02-08T06:26:20.114250Z","shell.execute_reply":"2024-02-08T06:26:20.112412Z","shell.execute_reply.started":"2024-02-08T06:26:20.102101Z"},"trusted":true},"outputs":[{"data":{"text/plain":["154    [input_ids, attention_mask]\n","93     [input_ids, attention_mask]\n","216    [input_ids, attention_mask]\n","217    [input_ids, attention_mask]\n","15     [input_ids, attention_mask]\n","                  ...             \n","106    [input_ids, attention_mask]\n","14     [input_ids, attention_mask]\n","92     [input_ids, attention_mask]\n","179    [input_ids, attention_mask]\n","102    [input_ids, attention_mask]\n","Length: 219, dtype: object"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T06:26:20.116300Z","iopub.status.busy":"2024-02-08T06:26:20.115958Z","iopub.status.idle":"2024-02-08T07:25:12.567867Z","shell.execute_reply":"2024-02-08T07:25:12.565542Z","shell.execute_reply.started":"2024-02-08T06:26:20.116267Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='703' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 703/1500 58:45 < 1:06:48, 0.20 it/s, Epoch 12.82/28]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>4.284900</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>4.446500</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>4.139200</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>4.106900</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>4.846500</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>4.832400</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>4.614600</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>3.820600</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>3.932300</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>4.598900</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>4.704900</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>4.530000</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>3.813800</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>4.392400</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>3.546600</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>4.117400</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>3.757400</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>2.756900</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>3.732700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>4.411000</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>4.514500</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>5.215800</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>4.753100</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>4.249100</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>4.760000</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>4.172300</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>3.765000</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>4.251500</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>3.752500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>3.583100</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>2.971800</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>4.044200</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>3.560000</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>3.808200</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>3.929400</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>3.217500</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>3.075700</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>2.775800</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>3.370400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>3.405700</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>3.211900</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>3.263900</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>3.116400</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>2.992200</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>3.241100</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>2.785100</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>3.022900</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>2.812800</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>2.677100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.258600</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>2.699400</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>2.714300</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>2.874500</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>2.493900</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>2.137900</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>2.780700</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>2.427400</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>2.127500</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>1.991300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.443800</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>1.989300</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>2.235700</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>2.234400</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>2.167700</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>1.834300</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>2.032100</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>1.971100</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>2.183200</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>1.585800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.499200</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>1.518300</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>1.892800</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>1.887500</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>1.538400</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.746700</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>2.160200</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>2.016500</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>1.538000</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>1.672400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.594800</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>1.579800</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>1.404100</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>1.749100</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>1.469000</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>1.511800</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>1.486500</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>1.649200</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>2.498900</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>1.554100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.482200</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>1.349600</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>1.162600</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>1.405100</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>1.477300</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>1.452600</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>1.174200</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>1.707700</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>1.185900</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>1.203500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.860500</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>1.389000</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>1.244800</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>1.281700</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>1.342700</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>1.709900</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>1.526600</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>1.077500</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>1.219300</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>1.229300</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.865800</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>1.373800</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>1.067700</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>1.261900</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>1.959700</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>1.102300</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>1.327500</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>0.889400</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>0.732700</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>0.869200</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.973600</td>\n","    </tr>\n","    <tr>\n","      <td>121</td>\n","      <td>0.917300</td>\n","    </tr>\n","    <tr>\n","      <td>122</td>\n","      <td>0.817000</td>\n","    </tr>\n","    <tr>\n","      <td>123</td>\n","      <td>1.269600</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>0.737200</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.913500</td>\n","    </tr>\n","    <tr>\n","      <td>126</td>\n","      <td>0.846800</td>\n","    </tr>\n","    <tr>\n","      <td>127</td>\n","      <td>0.888500</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>0.785400</td>\n","    </tr>\n","    <tr>\n","      <td>129</td>\n","      <td>0.866600</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>1.093700</td>\n","    </tr>\n","    <tr>\n","      <td>131</td>\n","      <td>1.499700</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>1.104100</td>\n","    </tr>\n","    <tr>\n","      <td>133</td>\n","      <td>0.680300</td>\n","    </tr>\n","    <tr>\n","      <td>134</td>\n","      <td>0.806300</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>1.221300</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>0.827200</td>\n","    </tr>\n","    <tr>\n","      <td>137</td>\n","      <td>1.241100</td>\n","    </tr>\n","    <tr>\n","      <td>138</td>\n","      <td>0.769700</td>\n","    </tr>\n","    <tr>\n","      <td>139</td>\n","      <td>0.990200</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>1.308800</td>\n","    </tr>\n","    <tr>\n","      <td>141</td>\n","      <td>0.594300</td>\n","    </tr>\n","    <tr>\n","      <td>142</td>\n","      <td>1.171900</td>\n","    </tr>\n","    <tr>\n","      <td>143</td>\n","      <td>0.977100</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.930100</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>1.399900</td>\n","    </tr>\n","    <tr>\n","      <td>146</td>\n","      <td>1.492800</td>\n","    </tr>\n","    <tr>\n","      <td>147</td>\n","      <td>1.124600</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>1.500600</td>\n","    </tr>\n","    <tr>\n","      <td>149</td>\n","      <td>1.049100</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.635300</td>\n","    </tr>\n","    <tr>\n","      <td>151</td>\n","      <td>0.902000</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>0.730300</td>\n","    </tr>\n","    <tr>\n","      <td>153</td>\n","      <td>1.094400</td>\n","    </tr>\n","    <tr>\n","      <td>154</td>\n","      <td>1.023800</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.902700</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>0.721600</td>\n","    </tr>\n","    <tr>\n","      <td>157</td>\n","      <td>0.353000</td>\n","    </tr>\n","    <tr>\n","      <td>158</td>\n","      <td>0.949500</td>\n","    </tr>\n","    <tr>\n","      <td>159</td>\n","      <td>0.582900</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.531300</td>\n","    </tr>\n","    <tr>\n","      <td>161</td>\n","      <td>0.554700</td>\n","    </tr>\n","    <tr>\n","      <td>162</td>\n","      <td>1.921500</td>\n","    </tr>\n","    <tr>\n","      <td>163</td>\n","      <td>0.752400</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>0.915700</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.735000</td>\n","    </tr>\n","    <tr>\n","      <td>166</td>\n","      <td>0.519500</td>\n","    </tr>\n","    <tr>\n","      <td>167</td>\n","      <td>0.924000</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>1.003000</td>\n","    </tr>\n","    <tr>\n","      <td>169</td>\n","      <td>0.900900</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.683300</td>\n","    </tr>\n","    <tr>\n","      <td>171</td>\n","      <td>0.495300</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>0.450600</td>\n","    </tr>\n","    <tr>\n","      <td>173</td>\n","      <td>0.507300</td>\n","    </tr>\n","    <tr>\n","      <td>174</td>\n","      <td>0.402200</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.588100</td>\n","    </tr>\n","    <tr>\n","      <td>176</td>\n","      <td>0.732200</td>\n","    </tr>\n","    <tr>\n","      <td>177</td>\n","      <td>0.317400</td>\n","    </tr>\n","    <tr>\n","      <td>178</td>\n","      <td>0.675900</td>\n","    </tr>\n","    <tr>\n","      <td>179</td>\n","      <td>0.535400</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.744900</td>\n","    </tr>\n","    <tr>\n","      <td>181</td>\n","      <td>0.439600</td>\n","    </tr>\n","    <tr>\n","      <td>182</td>\n","      <td>0.613900</td>\n","    </tr>\n","    <tr>\n","      <td>183</td>\n","      <td>0.855100</td>\n","    </tr>\n","    <tr>\n","      <td>184</td>\n","      <td>0.456500</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>0.723000</td>\n","    </tr>\n","    <tr>\n","      <td>186</td>\n","      <td>0.410500</td>\n","    </tr>\n","    <tr>\n","      <td>187</td>\n","      <td>0.511800</td>\n","    </tr>\n","    <tr>\n","      <td>188</td>\n","      <td>0.683800</td>\n","    </tr>\n","    <tr>\n","      <td>189</td>\n","      <td>1.014800</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.389600</td>\n","    </tr>\n","    <tr>\n","      <td>191</td>\n","      <td>0.358900</td>\n","    </tr>\n","    <tr>\n","      <td>192</td>\n","      <td>0.607700</td>\n","    </tr>\n","    <tr>\n","      <td>193</td>\n","      <td>1.273400</td>\n","    </tr>\n","    <tr>\n","      <td>194</td>\n","      <td>1.383800</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.481200</td>\n","    </tr>\n","    <tr>\n","      <td>196</td>\n","      <td>0.519400</td>\n","    </tr>\n","    <tr>\n","      <td>197</td>\n","      <td>0.605400</td>\n","    </tr>\n","    <tr>\n","      <td>198</td>\n","      <td>0.848400</td>\n","    </tr>\n","    <tr>\n","      <td>199</td>\n","      <td>0.547300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.501900</td>\n","    </tr>\n","    <tr>\n","      <td>201</td>\n","      <td>0.665200</td>\n","    </tr>\n","    <tr>\n","      <td>202</td>\n","      <td>0.610500</td>\n","    </tr>\n","    <tr>\n","      <td>203</td>\n","      <td>0.497200</td>\n","    </tr>\n","    <tr>\n","      <td>204</td>\n","      <td>0.947200</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>0.854600</td>\n","    </tr>\n","    <tr>\n","      <td>206</td>\n","      <td>0.570000</td>\n","    </tr>\n","    <tr>\n","      <td>207</td>\n","      <td>0.565900</td>\n","    </tr>\n","    <tr>\n","      <td>208</td>\n","      <td>0.554600</td>\n","    </tr>\n","    <tr>\n","      <td>209</td>\n","      <td>0.780000</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.383600</td>\n","    </tr>\n","    <tr>\n","      <td>211</td>\n","      <td>0.547100</td>\n","    </tr>\n","    <tr>\n","      <td>212</td>\n","      <td>0.900900</td>\n","    </tr>\n","    <tr>\n","      <td>213</td>\n","      <td>0.903900</td>\n","    </tr>\n","    <tr>\n","      <td>214</td>\n","      <td>0.539100</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>1.026400</td>\n","    </tr>\n","    <tr>\n","      <td>216</td>\n","      <td>0.456500</td>\n","    </tr>\n","    <tr>\n","      <td>217</td>\n","      <td>0.517500</td>\n","    </tr>\n","    <tr>\n","      <td>218</td>\n","      <td>0.515400</td>\n","    </tr>\n","    <tr>\n","      <td>219</td>\n","      <td>0.458300</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.305800</td>\n","    </tr>\n","    <tr>\n","      <td>221</td>\n","      <td>0.318100</td>\n","    </tr>\n","    <tr>\n","      <td>222</td>\n","      <td>0.428000</td>\n","    </tr>\n","    <tr>\n","      <td>223</td>\n","      <td>0.599000</td>\n","    </tr>\n","    <tr>\n","      <td>224</td>\n","      <td>0.696100</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.047400</td>\n","    </tr>\n","    <tr>\n","      <td>226</td>\n","      <td>0.437200</td>\n","    </tr>\n","    <tr>\n","      <td>227</td>\n","      <td>0.349700</td>\n","    </tr>\n","    <tr>\n","      <td>228</td>\n","      <td>0.451300</td>\n","    </tr>\n","    <tr>\n","      <td>229</td>\n","      <td>0.398800</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.267100</td>\n","    </tr>\n","    <tr>\n","      <td>231</td>\n","      <td>0.681000</td>\n","    </tr>\n","    <tr>\n","      <td>232</td>\n","      <td>0.507600</td>\n","    </tr>\n","    <tr>\n","      <td>233</td>\n","      <td>0.606500</td>\n","    </tr>\n","    <tr>\n","      <td>234</td>\n","      <td>0.366000</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>0.360000</td>\n","    </tr>\n","    <tr>\n","      <td>236</td>\n","      <td>0.318000</td>\n","    </tr>\n","    <tr>\n","      <td>237</td>\n","      <td>0.395600</td>\n","    </tr>\n","    <tr>\n","      <td>238</td>\n","      <td>0.600100</td>\n","    </tr>\n","    <tr>\n","      <td>239</td>\n","      <td>0.413000</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.296300</td>\n","    </tr>\n","    <tr>\n","      <td>241</td>\n","      <td>0.322300</td>\n","    </tr>\n","    <tr>\n","      <td>242</td>\n","      <td>0.307900</td>\n","    </tr>\n","    <tr>\n","      <td>243</td>\n","      <td>0.646100</td>\n","    </tr>\n","    <tr>\n","      <td>244</td>\n","      <td>0.564300</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>0.467700</td>\n","    </tr>\n","    <tr>\n","      <td>246</td>\n","      <td>0.451800</td>\n","    </tr>\n","    <tr>\n","      <td>247</td>\n","      <td>0.416800</td>\n","    </tr>\n","    <tr>\n","      <td>248</td>\n","      <td>0.321300</td>\n","    </tr>\n","    <tr>\n","      <td>249</td>\n","      <td>0.332500</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.392200</td>\n","    </tr>\n","    <tr>\n","      <td>251</td>\n","      <td>0.469800</td>\n","    </tr>\n","    <tr>\n","      <td>252</td>\n","      <td>0.329900</td>\n","    </tr>\n","    <tr>\n","      <td>253</td>\n","      <td>0.213500</td>\n","    </tr>\n","    <tr>\n","      <td>254</td>\n","      <td>1.110900</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>0.456400</td>\n","    </tr>\n","    <tr>\n","      <td>256</td>\n","      <td>0.418500</td>\n","    </tr>\n","    <tr>\n","      <td>257</td>\n","      <td>0.583400</td>\n","    </tr>\n","    <tr>\n","      <td>258</td>\n","      <td>0.312000</td>\n","    </tr>\n","    <tr>\n","      <td>259</td>\n","      <td>0.406900</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.529500</td>\n","    </tr>\n","    <tr>\n","      <td>261</td>\n","      <td>0.748400</td>\n","    </tr>\n","    <tr>\n","      <td>262</td>\n","      <td>0.349600</td>\n","    </tr>\n","    <tr>\n","      <td>263</td>\n","      <td>0.484500</td>\n","    </tr>\n","    <tr>\n","      <td>264</td>\n","      <td>0.350900</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>0.327900</td>\n","    </tr>\n","    <tr>\n","      <td>266</td>\n","      <td>0.436700</td>\n","    </tr>\n","    <tr>\n","      <td>267</td>\n","      <td>0.384400</td>\n","    </tr>\n","    <tr>\n","      <td>268</td>\n","      <td>0.685000</td>\n","    </tr>\n","    <tr>\n","      <td>269</td>\n","      <td>0.493100</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.362000</td>\n","    </tr>\n","    <tr>\n","      <td>271</td>\n","      <td>0.411300</td>\n","    </tr>\n","    <tr>\n","      <td>272</td>\n","      <td>0.289700</td>\n","    </tr>\n","    <tr>\n","      <td>273</td>\n","      <td>0.267600</td>\n","    </tr>\n","    <tr>\n","      <td>274</td>\n","      <td>0.491700</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.236400</td>\n","    </tr>\n","    <tr>\n","      <td>276</td>\n","      <td>0.307200</td>\n","    </tr>\n","    <tr>\n","      <td>277</td>\n","      <td>0.323600</td>\n","    </tr>\n","    <tr>\n","      <td>278</td>\n","      <td>0.670000</td>\n","    </tr>\n","    <tr>\n","      <td>279</td>\n","      <td>0.349600</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.312800</td>\n","    </tr>\n","    <tr>\n","      <td>281</td>\n","      <td>0.364000</td>\n","    </tr>\n","    <tr>\n","      <td>282</td>\n","      <td>0.386300</td>\n","    </tr>\n","    <tr>\n","      <td>283</td>\n","      <td>0.351100</td>\n","    </tr>\n","    <tr>\n","      <td>284</td>\n","      <td>0.282200</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>0.379900</td>\n","    </tr>\n","    <tr>\n","      <td>286</td>\n","      <td>0.317900</td>\n","    </tr>\n","    <tr>\n","      <td>287</td>\n","      <td>0.300100</td>\n","    </tr>\n","    <tr>\n","      <td>288</td>\n","      <td>0.332400</td>\n","    </tr>\n","    <tr>\n","      <td>289</td>\n","      <td>0.939400</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.972500</td>\n","    </tr>\n","    <tr>\n","      <td>291</td>\n","      <td>0.501100</td>\n","    </tr>\n","    <tr>\n","      <td>292</td>\n","      <td>0.370800</td>\n","    </tr>\n","    <tr>\n","      <td>293</td>\n","      <td>0.368700</td>\n","    </tr>\n","    <tr>\n","      <td>294</td>\n","      <td>0.342400</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>0.545100</td>\n","    </tr>\n","    <tr>\n","      <td>296</td>\n","      <td>0.257500</td>\n","    </tr>\n","    <tr>\n","      <td>297</td>\n","      <td>0.368300</td>\n","    </tr>\n","    <tr>\n","      <td>298</td>\n","      <td>0.315900</td>\n","    </tr>\n","    <tr>\n","      <td>299</td>\n","      <td>0.208500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.200900</td>\n","    </tr>\n","    <tr>\n","      <td>301</td>\n","      <td>0.306000</td>\n","    </tr>\n","    <tr>\n","      <td>302</td>\n","      <td>0.272400</td>\n","    </tr>\n","    <tr>\n","      <td>303</td>\n","      <td>0.337100</td>\n","    </tr>\n","    <tr>\n","      <td>304</td>\n","      <td>0.295000</td>\n","    </tr>\n","    <tr>\n","      <td>305</td>\n","      <td>0.292600</td>\n","    </tr>\n","    <tr>\n","      <td>306</td>\n","      <td>0.352300</td>\n","    </tr>\n","    <tr>\n","      <td>307</td>\n","      <td>0.454300</td>\n","    </tr>\n","    <tr>\n","      <td>308</td>\n","      <td>0.286400</td>\n","    </tr>\n","    <tr>\n","      <td>309</td>\n","      <td>0.252300</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.284300</td>\n","    </tr>\n","    <tr>\n","      <td>311</td>\n","      <td>0.426400</td>\n","    </tr>\n","    <tr>\n","      <td>312</td>\n","      <td>0.345900</td>\n","    </tr>\n","    <tr>\n","      <td>313</td>\n","      <td>0.393400</td>\n","    </tr>\n","    <tr>\n","      <td>314</td>\n","      <td>0.283800</td>\n","    </tr>\n","    <tr>\n","      <td>315</td>\n","      <td>0.262900</td>\n","    </tr>\n","    <tr>\n","      <td>316</td>\n","      <td>0.352000</td>\n","    </tr>\n","    <tr>\n","      <td>317</td>\n","      <td>0.507200</td>\n","    </tr>\n","    <tr>\n","      <td>318</td>\n","      <td>0.365000</td>\n","    </tr>\n","    <tr>\n","      <td>319</td>\n","      <td>0.486000</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.362000</td>\n","    </tr>\n","    <tr>\n","      <td>321</td>\n","      <td>0.236600</td>\n","    </tr>\n","    <tr>\n","      <td>322</td>\n","      <td>0.344100</td>\n","    </tr>\n","    <tr>\n","      <td>323</td>\n","      <td>0.260000</td>\n","    </tr>\n","    <tr>\n","      <td>324</td>\n","      <td>0.341100</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>0.258500</td>\n","    </tr>\n","    <tr>\n","      <td>326</td>\n","      <td>0.265600</td>\n","    </tr>\n","    <tr>\n","      <td>327</td>\n","      <td>0.410300</td>\n","    </tr>\n","    <tr>\n","      <td>328</td>\n","      <td>0.318700</td>\n","    </tr>\n","    <tr>\n","      <td>329</td>\n","      <td>0.196300</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.861000</td>\n","    </tr>\n","    <tr>\n","      <td>331</td>\n","      <td>0.299200</td>\n","    </tr>\n","    <tr>\n","      <td>332</td>\n","      <td>0.206300</td>\n","    </tr>\n","    <tr>\n","      <td>333</td>\n","      <td>0.326900</td>\n","    </tr>\n","    <tr>\n","      <td>334</td>\n","      <td>0.296700</td>\n","    </tr>\n","    <tr>\n","      <td>335</td>\n","      <td>0.317500</td>\n","    </tr>\n","    <tr>\n","      <td>336</td>\n","      <td>0.263300</td>\n","    </tr>\n","    <tr>\n","      <td>337</td>\n","      <td>0.274600</td>\n","    </tr>\n","    <tr>\n","      <td>338</td>\n","      <td>0.244900</td>\n","    </tr>\n","    <tr>\n","      <td>339</td>\n","      <td>0.269100</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.415000</td>\n","    </tr>\n","    <tr>\n","      <td>341</td>\n","      <td>0.199700</td>\n","    </tr>\n","    <tr>\n","      <td>342</td>\n","      <td>0.328900</td>\n","    </tr>\n","    <tr>\n","      <td>343</td>\n","      <td>0.393500</td>\n","    </tr>\n","    <tr>\n","      <td>344</td>\n","      <td>0.335700</td>\n","    </tr>\n","    <tr>\n","      <td>345</td>\n","      <td>0.176300</td>\n","    </tr>\n","    <tr>\n","      <td>346</td>\n","      <td>0.330800</td>\n","    </tr>\n","    <tr>\n","      <td>347</td>\n","      <td>0.305000</td>\n","    </tr>\n","    <tr>\n","      <td>348</td>\n","      <td>0.231900</td>\n","    </tr>\n","    <tr>\n","      <td>349</td>\n","      <td>0.367700</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.272800</td>\n","    </tr>\n","    <tr>\n","      <td>351</td>\n","      <td>0.367300</td>\n","    </tr>\n","    <tr>\n","      <td>352</td>\n","      <td>0.293900</td>\n","    </tr>\n","    <tr>\n","      <td>353</td>\n","      <td>0.337300</td>\n","    </tr>\n","    <tr>\n","      <td>354</td>\n","      <td>0.362500</td>\n","    </tr>\n","    <tr>\n","      <td>355</td>\n","      <td>0.312200</td>\n","    </tr>\n","    <tr>\n","      <td>356</td>\n","      <td>0.258800</td>\n","    </tr>\n","    <tr>\n","      <td>357</td>\n","      <td>0.256000</td>\n","    </tr>\n","    <tr>\n","      <td>358</td>\n","      <td>0.271200</td>\n","    </tr>\n","    <tr>\n","      <td>359</td>\n","      <td>0.387100</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.235600</td>\n","    </tr>\n","    <tr>\n","      <td>361</td>\n","      <td>0.259200</td>\n","    </tr>\n","    <tr>\n","      <td>362</td>\n","      <td>0.201100</td>\n","    </tr>\n","    <tr>\n","      <td>363</td>\n","      <td>0.308900</td>\n","    </tr>\n","    <tr>\n","      <td>364</td>\n","      <td>0.260400</td>\n","    </tr>\n","    <tr>\n","      <td>365</td>\n","      <td>0.319700</td>\n","    </tr>\n","    <tr>\n","      <td>366</td>\n","      <td>0.264800</td>\n","    </tr>\n","    <tr>\n","      <td>367</td>\n","      <td>0.397100</td>\n","    </tr>\n","    <tr>\n","      <td>368</td>\n","      <td>0.374200</td>\n","    </tr>\n","    <tr>\n","      <td>369</td>\n","      <td>0.262700</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.325300</td>\n","    </tr>\n","    <tr>\n","      <td>371</td>\n","      <td>0.960600</td>\n","    </tr>\n","    <tr>\n","      <td>372</td>\n","      <td>0.376700</td>\n","    </tr>\n","    <tr>\n","      <td>373</td>\n","      <td>0.283900</td>\n","    </tr>\n","    <tr>\n","      <td>374</td>\n","      <td>0.234900</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>0.300700</td>\n","    </tr>\n","    <tr>\n","      <td>376</td>\n","      <td>0.313700</td>\n","    </tr>\n","    <tr>\n","      <td>377</td>\n","      <td>0.286400</td>\n","    </tr>\n","    <tr>\n","      <td>378</td>\n","      <td>0.235900</td>\n","    </tr>\n","    <tr>\n","      <td>379</td>\n","      <td>0.292600</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.288800</td>\n","    </tr>\n","    <tr>\n","      <td>381</td>\n","      <td>0.267500</td>\n","    </tr>\n","    <tr>\n","      <td>382</td>\n","      <td>0.319000</td>\n","    </tr>\n","    <tr>\n","      <td>383</td>\n","      <td>0.220800</td>\n","    </tr>\n","    <tr>\n","      <td>384</td>\n","      <td>0.258500</td>\n","    </tr>\n","    <tr>\n","      <td>385</td>\n","      <td>0.204600</td>\n","    </tr>\n","    <tr>\n","      <td>386</td>\n","      <td>0.209700</td>\n","    </tr>\n","    <tr>\n","      <td>387</td>\n","      <td>0.230700</td>\n","    </tr>\n","    <tr>\n","      <td>388</td>\n","      <td>0.142700</td>\n","    </tr>\n","    <tr>\n","      <td>389</td>\n","      <td>0.256300</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.177300</td>\n","    </tr>\n","    <tr>\n","      <td>391</td>\n","      <td>0.273600</td>\n","    </tr>\n","    <tr>\n","      <td>392</td>\n","      <td>0.358500</td>\n","    </tr>\n","    <tr>\n","      <td>393</td>\n","      <td>0.272600</td>\n","    </tr>\n","    <tr>\n","      <td>394</td>\n","      <td>0.941400</td>\n","    </tr>\n","    <tr>\n","      <td>395</td>\n","      <td>0.230800</td>\n","    </tr>\n","    <tr>\n","      <td>396</td>\n","      <td>0.169500</td>\n","    </tr>\n","    <tr>\n","      <td>397</td>\n","      <td>0.301000</td>\n","    </tr>\n","    <tr>\n","      <td>398</td>\n","      <td>0.267100</td>\n","    </tr>\n","    <tr>\n","      <td>399</td>\n","      <td>0.276300</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.319900</td>\n","    </tr>\n","    <tr>\n","      <td>401</td>\n","      <td>0.299800</td>\n","    </tr>\n","    <tr>\n","      <td>402</td>\n","      <td>0.241100</td>\n","    </tr>\n","    <tr>\n","      <td>403</td>\n","      <td>0.253700</td>\n","    </tr>\n","    <tr>\n","      <td>404</td>\n","      <td>0.334200</td>\n","    </tr>\n","    <tr>\n","      <td>405</td>\n","      <td>0.266500</td>\n","    </tr>\n","    <tr>\n","      <td>406</td>\n","      <td>0.183000</td>\n","    </tr>\n","    <tr>\n","      <td>407</td>\n","      <td>0.230500</td>\n","    </tr>\n","    <tr>\n","      <td>408</td>\n","      <td>0.231300</td>\n","    </tr>\n","    <tr>\n","      <td>409</td>\n","      <td>0.244100</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.280300</td>\n","    </tr>\n","    <tr>\n","      <td>411</td>\n","      <td>0.279600</td>\n","    </tr>\n","    <tr>\n","      <td>412</td>\n","      <td>0.313700</td>\n","    </tr>\n","    <tr>\n","      <td>413</td>\n","      <td>0.272300</td>\n","    </tr>\n","    <tr>\n","      <td>414</td>\n","      <td>0.217100</td>\n","    </tr>\n","    <tr>\n","      <td>415</td>\n","      <td>0.191900</td>\n","    </tr>\n","    <tr>\n","      <td>416</td>\n","      <td>0.287500</td>\n","    </tr>\n","    <tr>\n","      <td>417</td>\n","      <td>0.333100</td>\n","    </tr>\n","    <tr>\n","      <td>418</td>\n","      <td>0.270100</td>\n","    </tr>\n","    <tr>\n","      <td>419</td>\n","      <td>0.194300</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.339300</td>\n","    </tr>\n","    <tr>\n","      <td>421</td>\n","      <td>0.219500</td>\n","    </tr>\n","    <tr>\n","      <td>422</td>\n","      <td>0.312400</td>\n","    </tr>\n","    <tr>\n","      <td>423</td>\n","      <td>0.319700</td>\n","    </tr>\n","    <tr>\n","      <td>424</td>\n","      <td>0.222800</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>0.227100</td>\n","    </tr>\n","    <tr>\n","      <td>426</td>\n","      <td>0.263100</td>\n","    </tr>\n","    <tr>\n","      <td>427</td>\n","      <td>0.241500</td>\n","    </tr>\n","    <tr>\n","      <td>428</td>\n","      <td>0.266500</td>\n","    </tr>\n","    <tr>\n","      <td>429</td>\n","      <td>0.261700</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.191300</td>\n","    </tr>\n","    <tr>\n","      <td>431</td>\n","      <td>0.255100</td>\n","    </tr>\n","    <tr>\n","      <td>432</td>\n","      <td>0.215800</td>\n","    </tr>\n","    <tr>\n","      <td>433</td>\n","      <td>0.325700</td>\n","    </tr>\n","    <tr>\n","      <td>434</td>\n","      <td>0.904500</td>\n","    </tr>\n","    <tr>\n","      <td>435</td>\n","      <td>0.339400</td>\n","    </tr>\n","    <tr>\n","      <td>436</td>\n","      <td>0.197800</td>\n","    </tr>\n","    <tr>\n","      <td>437</td>\n","      <td>0.292400</td>\n","    </tr>\n","    <tr>\n","      <td>438</td>\n","      <td>0.327600</td>\n","    </tr>\n","    <tr>\n","      <td>439</td>\n","      <td>0.283900</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.274200</td>\n","    </tr>\n","    <tr>\n","      <td>441</td>\n","      <td>0.227900</td>\n","    </tr>\n","    <tr>\n","      <td>442</td>\n","      <td>0.240200</td>\n","    </tr>\n","    <tr>\n","      <td>443</td>\n","      <td>0.236000</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>0.239400</td>\n","    </tr>\n","    <tr>\n","      <td>445</td>\n","      <td>0.222700</td>\n","    </tr>\n","    <tr>\n","      <td>446</td>\n","      <td>0.233600</td>\n","    </tr>\n","    <tr>\n","      <td>447</td>\n","      <td>0.195200</td>\n","    </tr>\n","    <tr>\n","      <td>448</td>\n","      <td>0.251700</td>\n","    </tr>\n","    <tr>\n","      <td>449</td>\n","      <td>0.200500</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.223400</td>\n","    </tr>\n","    <tr>\n","      <td>451</td>\n","      <td>0.282800</td>\n","    </tr>\n","    <tr>\n","      <td>452</td>\n","      <td>0.195100</td>\n","    </tr>\n","    <tr>\n","      <td>453</td>\n","      <td>0.267800</td>\n","    </tr>\n","    <tr>\n","      <td>454</td>\n","      <td>0.211000</td>\n","    </tr>\n","    <tr>\n","      <td>455</td>\n","      <td>0.206500</td>\n","    </tr>\n","    <tr>\n","      <td>456</td>\n","      <td>0.145800</td>\n","    </tr>\n","    <tr>\n","      <td>457</td>\n","      <td>0.171000</td>\n","    </tr>\n","    <tr>\n","      <td>458</td>\n","      <td>0.233400</td>\n","    </tr>\n","    <tr>\n","      <td>459</td>\n","      <td>0.225100</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.203900</td>\n","    </tr>\n","    <tr>\n","      <td>461</td>\n","      <td>0.194200</td>\n","    </tr>\n","    <tr>\n","      <td>462</td>\n","      <td>0.201000</td>\n","    </tr>\n","    <tr>\n","      <td>463</td>\n","      <td>0.859000</td>\n","    </tr>\n","    <tr>\n","      <td>464</td>\n","      <td>0.193900</td>\n","    </tr>\n","    <tr>\n","      <td>465</td>\n","      <td>0.317800</td>\n","    </tr>\n","    <tr>\n","      <td>466</td>\n","      <td>0.308300</td>\n","    </tr>\n","    <tr>\n","      <td>467</td>\n","      <td>0.199400</td>\n","    </tr>\n","    <tr>\n","      <td>468</td>\n","      <td>0.219500</td>\n","    </tr>\n","    <tr>\n","      <td>469</td>\n","      <td>0.180400</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.292700</td>\n","    </tr>\n","    <tr>\n","      <td>471</td>\n","      <td>0.297100</td>\n","    </tr>\n","    <tr>\n","      <td>472</td>\n","      <td>0.299400</td>\n","    </tr>\n","    <tr>\n","      <td>473</td>\n","      <td>0.128700</td>\n","    </tr>\n","    <tr>\n","      <td>474</td>\n","      <td>0.274300</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>0.172100</td>\n","    </tr>\n","    <tr>\n","      <td>476</td>\n","      <td>0.939200</td>\n","    </tr>\n","    <tr>\n","      <td>477</td>\n","      <td>0.305900</td>\n","    </tr>\n","    <tr>\n","      <td>478</td>\n","      <td>0.320500</td>\n","    </tr>\n","    <tr>\n","      <td>479</td>\n","      <td>0.253500</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.217900</td>\n","    </tr>\n","    <tr>\n","      <td>481</td>\n","      <td>0.257500</td>\n","    </tr>\n","    <tr>\n","      <td>482</td>\n","      <td>0.303300</td>\n","    </tr>\n","    <tr>\n","      <td>483</td>\n","      <td>0.188900</td>\n","    </tr>\n","    <tr>\n","      <td>484</td>\n","      <td>0.279300</td>\n","    </tr>\n","    <tr>\n","      <td>485</td>\n","      <td>0.269300</td>\n","    </tr>\n","    <tr>\n","      <td>486</td>\n","      <td>0.208800</td>\n","    </tr>\n","    <tr>\n","      <td>487</td>\n","      <td>0.173800</td>\n","    </tr>\n","    <tr>\n","      <td>488</td>\n","      <td>0.288500</td>\n","    </tr>\n","    <tr>\n","      <td>489</td>\n","      <td>0.296100</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.263300</td>\n","    </tr>\n","    <tr>\n","      <td>491</td>\n","      <td>0.242700</td>\n","    </tr>\n","    <tr>\n","      <td>492</td>\n","      <td>0.287700</td>\n","    </tr>\n","    <tr>\n","      <td>493</td>\n","      <td>0.303600</td>\n","    </tr>\n","    <tr>\n","      <td>494</td>\n","      <td>0.292700</td>\n","    </tr>\n","    <tr>\n","      <td>495</td>\n","      <td>0.209600</td>\n","    </tr>\n","    <tr>\n","      <td>496</td>\n","      <td>0.168100</td>\n","    </tr>\n","    <tr>\n","      <td>497</td>\n","      <td>0.309200</td>\n","    </tr>\n","    <tr>\n","      <td>498</td>\n","      <td>0.239400</td>\n","    </tr>\n","    <tr>\n","      <td>499</td>\n","      <td>0.244200</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.196100</td>\n","    </tr>\n","    <tr>\n","      <td>501</td>\n","      <td>0.196500</td>\n","    </tr>\n","    <tr>\n","      <td>502</td>\n","      <td>0.199900</td>\n","    </tr>\n","    <tr>\n","      <td>503</td>\n","      <td>0.180200</td>\n","    </tr>\n","    <tr>\n","      <td>504</td>\n","      <td>0.200400</td>\n","    </tr>\n","    <tr>\n","      <td>505</td>\n","      <td>0.217100</td>\n","    </tr>\n","    <tr>\n","      <td>506</td>\n","      <td>0.264300</td>\n","    </tr>\n","    <tr>\n","      <td>507</td>\n","      <td>0.286200</td>\n","    </tr>\n","    <tr>\n","      <td>508</td>\n","      <td>0.231700</td>\n","    </tr>\n","    <tr>\n","      <td>509</td>\n","      <td>0.212800</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.191800</td>\n","    </tr>\n","    <tr>\n","      <td>511</td>\n","      <td>0.217300</td>\n","    </tr>\n","    <tr>\n","      <td>512</td>\n","      <td>0.229200</td>\n","    </tr>\n","    <tr>\n","      <td>513</td>\n","      <td>0.169200</td>\n","    </tr>\n","    <tr>\n","      <td>514</td>\n","      <td>0.296100</td>\n","    </tr>\n","    <tr>\n","      <td>515</td>\n","      <td>0.230500</td>\n","    </tr>\n","    <tr>\n","      <td>516</td>\n","      <td>0.215900</td>\n","    </tr>\n","    <tr>\n","      <td>517</td>\n","      <td>0.164500</td>\n","    </tr>\n","    <tr>\n","      <td>518</td>\n","      <td>0.244600</td>\n","    </tr>\n","    <tr>\n","      <td>519</td>\n","      <td>0.274300</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.271100</td>\n","    </tr>\n","    <tr>\n","      <td>521</td>\n","      <td>0.245500</td>\n","    </tr>\n","    <tr>\n","      <td>522</td>\n","      <td>0.669900</td>\n","    </tr>\n","    <tr>\n","      <td>523</td>\n","      <td>0.277700</td>\n","    </tr>\n","    <tr>\n","      <td>524</td>\n","      <td>0.276700</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>0.196900</td>\n","    </tr>\n","    <tr>\n","      <td>526</td>\n","      <td>0.246300</td>\n","    </tr>\n","    <tr>\n","      <td>527</td>\n","      <td>0.206800</td>\n","    </tr>\n","    <tr>\n","      <td>528</td>\n","      <td>0.269600</td>\n","    </tr>\n","    <tr>\n","      <td>529</td>\n","      <td>0.262600</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.232900</td>\n","    </tr>\n","    <tr>\n","      <td>531</td>\n","      <td>0.250100</td>\n","    </tr>\n","    <tr>\n","      <td>532</td>\n","      <td>0.211400</td>\n","    </tr>\n","    <tr>\n","      <td>533</td>\n","      <td>0.188200</td>\n","    </tr>\n","    <tr>\n","      <td>534</td>\n","      <td>0.218600</td>\n","    </tr>\n","    <tr>\n","      <td>535</td>\n","      <td>0.268900</td>\n","    </tr>\n","    <tr>\n","      <td>536</td>\n","      <td>0.293700</td>\n","    </tr>\n","    <tr>\n","      <td>537</td>\n","      <td>0.282900</td>\n","    </tr>\n","    <tr>\n","      <td>538</td>\n","      <td>0.253000</td>\n","    </tr>\n","    <tr>\n","      <td>539</td>\n","      <td>0.177600</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.170600</td>\n","    </tr>\n","    <tr>\n","      <td>541</td>\n","      <td>0.103100</td>\n","    </tr>\n","    <tr>\n","      <td>542</td>\n","      <td>0.651000</td>\n","    </tr>\n","    <tr>\n","      <td>543</td>\n","      <td>0.214300</td>\n","    </tr>\n","    <tr>\n","      <td>544</td>\n","      <td>0.370600</td>\n","    </tr>\n","    <tr>\n","      <td>545</td>\n","      <td>0.289800</td>\n","    </tr>\n","    <tr>\n","      <td>546</td>\n","      <td>0.232900</td>\n","    </tr>\n","    <tr>\n","      <td>547</td>\n","      <td>0.167200</td>\n","    </tr>\n","    <tr>\n","      <td>548</td>\n","      <td>0.216100</td>\n","    </tr>\n","    <tr>\n","      <td>549</td>\n","      <td>0.189600</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.207400</td>\n","    </tr>\n","    <tr>\n","      <td>551</td>\n","      <td>0.189700</td>\n","    </tr>\n","    <tr>\n","      <td>552</td>\n","      <td>0.231200</td>\n","    </tr>\n","    <tr>\n","      <td>553</td>\n","      <td>0.241300</td>\n","    </tr>\n","    <tr>\n","      <td>554</td>\n","      <td>0.230100</td>\n","    </tr>\n","    <tr>\n","      <td>555</td>\n","      <td>0.302500</td>\n","    </tr>\n","    <tr>\n","      <td>556</td>\n","      <td>0.231300</td>\n","    </tr>\n","    <tr>\n","      <td>557</td>\n","      <td>0.236400</td>\n","    </tr>\n","    <tr>\n","      <td>558</td>\n","      <td>0.133000</td>\n","    </tr>\n","    <tr>\n","      <td>559</td>\n","      <td>0.218700</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.133200</td>\n","    </tr>\n","    <tr>\n","      <td>561</td>\n","      <td>0.304600</td>\n","    </tr>\n","    <tr>\n","      <td>562</td>\n","      <td>0.222600</td>\n","    </tr>\n","    <tr>\n","      <td>563</td>\n","      <td>0.164600</td>\n","    </tr>\n","    <tr>\n","      <td>564</td>\n","      <td>0.202900</td>\n","    </tr>\n","    <tr>\n","      <td>565</td>\n","      <td>0.280400</td>\n","    </tr>\n","    <tr>\n","      <td>566</td>\n","      <td>0.263400</td>\n","    </tr>\n","    <tr>\n","      <td>567</td>\n","      <td>0.167100</td>\n","    </tr>\n","    <tr>\n","      <td>568</td>\n","      <td>0.236600</td>\n","    </tr>\n","    <tr>\n","      <td>569</td>\n","      <td>0.263200</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.258700</td>\n","    </tr>\n","    <tr>\n","      <td>571</td>\n","      <td>0.275300</td>\n","    </tr>\n","    <tr>\n","      <td>572</td>\n","      <td>0.209300</td>\n","    </tr>\n","    <tr>\n","      <td>573</td>\n","      <td>0.531600</td>\n","    </tr>\n","    <tr>\n","      <td>574</td>\n","      <td>0.246200</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>0.176900</td>\n","    </tr>\n","    <tr>\n","      <td>576</td>\n","      <td>0.190100</td>\n","    </tr>\n","    <tr>\n","      <td>577</td>\n","      <td>0.283900</td>\n","    </tr>\n","    <tr>\n","      <td>578</td>\n","      <td>0.209000</td>\n","    </tr>\n","    <tr>\n","      <td>579</td>\n","      <td>0.301200</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.202000</td>\n","    </tr>\n","    <tr>\n","      <td>581</td>\n","      <td>0.238100</td>\n","    </tr>\n","    <tr>\n","      <td>582</td>\n","      <td>0.212600</td>\n","    </tr>\n","    <tr>\n","      <td>583</td>\n","      <td>0.260200</td>\n","    </tr>\n","    <tr>\n","      <td>584</td>\n","      <td>0.294900</td>\n","    </tr>\n","    <tr>\n","      <td>585</td>\n","      <td>0.269900</td>\n","    </tr>\n","    <tr>\n","      <td>586</td>\n","      <td>0.209800</td>\n","    </tr>\n","    <tr>\n","      <td>587</td>\n","      <td>0.233200</td>\n","    </tr>\n","    <tr>\n","      <td>588</td>\n","      <td>0.229600</td>\n","    </tr>\n","    <tr>\n","      <td>589</td>\n","      <td>0.215300</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>0.163000</td>\n","    </tr>\n","    <tr>\n","      <td>591</td>\n","      <td>0.151400</td>\n","    </tr>\n","    <tr>\n","      <td>592</td>\n","      <td>0.192500</td>\n","    </tr>\n","    <tr>\n","      <td>593</td>\n","      <td>0.431100</td>\n","    </tr>\n","    <tr>\n","      <td>594</td>\n","      <td>0.114000</td>\n","    </tr>\n","    <tr>\n","      <td>595</td>\n","      <td>0.196200</td>\n","    </tr>\n","    <tr>\n","      <td>596</td>\n","      <td>0.196900</td>\n","    </tr>\n","    <tr>\n","      <td>597</td>\n","      <td>0.188300</td>\n","    </tr>\n","    <tr>\n","      <td>598</td>\n","      <td>0.283700</td>\n","    </tr>\n","    <tr>\n","      <td>599</td>\n","      <td>0.314900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.172500</td>\n","    </tr>\n","    <tr>\n","      <td>601</td>\n","      <td>0.237400</td>\n","    </tr>\n","    <tr>\n","      <td>602</td>\n","      <td>0.259600</td>\n","    </tr>\n","    <tr>\n","      <td>603</td>\n","      <td>0.240600</td>\n","    </tr>\n","    <tr>\n","      <td>604</td>\n","      <td>0.220000</td>\n","    </tr>\n","    <tr>\n","      <td>605</td>\n","      <td>0.210400</td>\n","    </tr>\n","    <tr>\n","      <td>606</td>\n","      <td>0.245100</td>\n","    </tr>\n","    <tr>\n","      <td>607</td>\n","      <td>0.270000</td>\n","    </tr>\n","    <tr>\n","      <td>608</td>\n","      <td>0.182000</td>\n","    </tr>\n","    <tr>\n","      <td>609</td>\n","      <td>0.177800</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>0.200900</td>\n","    </tr>\n","    <tr>\n","      <td>611</td>\n","      <td>0.244500</td>\n","    </tr>\n","    <tr>\n","      <td>612</td>\n","      <td>0.218300</td>\n","    </tr>\n","    <tr>\n","      <td>613</td>\n","      <td>0.207900</td>\n","    </tr>\n","    <tr>\n","      <td>614</td>\n","      <td>0.222700</td>\n","    </tr>\n","    <tr>\n","      <td>615</td>\n","      <td>0.150200</td>\n","    </tr>\n","    <tr>\n","      <td>616</td>\n","      <td>0.189300</td>\n","    </tr>\n","    <tr>\n","      <td>617</td>\n","      <td>0.246700</td>\n","    </tr>\n","    <tr>\n","      <td>618</td>\n","      <td>0.206100</td>\n","    </tr>\n","    <tr>\n","      <td>619</td>\n","      <td>0.238000</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.220900</td>\n","    </tr>\n","    <tr>\n","      <td>621</td>\n","      <td>0.198900</td>\n","    </tr>\n","    <tr>\n","      <td>622</td>\n","      <td>0.248900</td>\n","    </tr>\n","    <tr>\n","      <td>623</td>\n","      <td>0.181200</td>\n","    </tr>\n","    <tr>\n","      <td>624</td>\n","      <td>0.183800</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>0.192400</td>\n","    </tr>\n","    <tr>\n","      <td>626</td>\n","      <td>0.209900</td>\n","    </tr>\n","    <tr>\n","      <td>627</td>\n","      <td>0.188100</td>\n","    </tr>\n","    <tr>\n","      <td>628</td>\n","      <td>0.222100</td>\n","    </tr>\n","    <tr>\n","      <td>629</td>\n","      <td>0.439800</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.236200</td>\n","    </tr>\n","    <tr>\n","      <td>631</td>\n","      <td>0.186100</td>\n","    </tr>\n","    <tr>\n","      <td>632</td>\n","      <td>0.168200</td>\n","    </tr>\n","    <tr>\n","      <td>633</td>\n","      <td>0.213900</td>\n","    </tr>\n","    <tr>\n","      <td>634</td>\n","      <td>0.158700</td>\n","    </tr>\n","    <tr>\n","      <td>635</td>\n","      <td>0.140000</td>\n","    </tr>\n","    <tr>\n","      <td>636</td>\n","      <td>0.255000</td>\n","    </tr>\n","    <tr>\n","      <td>637</td>\n","      <td>0.191800</td>\n","    </tr>\n","    <tr>\n","      <td>638</td>\n","      <td>0.271800</td>\n","    </tr>\n","    <tr>\n","      <td>639</td>\n","      <td>0.195900</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.263200</td>\n","    </tr>\n","    <tr>\n","      <td>641</td>\n","      <td>0.236400</td>\n","    </tr>\n","    <tr>\n","      <td>642</td>\n","      <td>0.248500</td>\n","    </tr>\n","    <tr>\n","      <td>643</td>\n","      <td>0.301700</td>\n","    </tr>\n","    <tr>\n","      <td>644</td>\n","      <td>0.299100</td>\n","    </tr>\n","    <tr>\n","      <td>645</td>\n","      <td>0.152200</td>\n","    </tr>\n","    <tr>\n","      <td>646</td>\n","      <td>0.166800</td>\n","    </tr>\n","    <tr>\n","      <td>647</td>\n","      <td>0.222300</td>\n","    </tr>\n","    <tr>\n","      <td>648</td>\n","      <td>0.347100</td>\n","    </tr>\n","    <tr>\n","      <td>649</td>\n","      <td>0.280600</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.236500</td>\n","    </tr>\n","    <tr>\n","      <td>651</td>\n","      <td>0.206500</td>\n","    </tr>\n","    <tr>\n","      <td>652</td>\n","      <td>0.190700</td>\n","    </tr>\n","    <tr>\n","      <td>653</td>\n","      <td>0.218000</td>\n","    </tr>\n","    <tr>\n","      <td>654</td>\n","      <td>0.158700</td>\n","    </tr>\n","    <tr>\n","      <td>655</td>\n","      <td>0.189000</td>\n","    </tr>\n","    <tr>\n","      <td>656</td>\n","      <td>0.222200</td>\n","    </tr>\n","    <tr>\n","      <td>657</td>\n","      <td>0.281800</td>\n","    </tr>\n","    <tr>\n","      <td>658</td>\n","      <td>0.222600</td>\n","    </tr>\n","    <tr>\n","      <td>659</td>\n","      <td>0.166600</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.166700</td>\n","    </tr>\n","    <tr>\n","      <td>661</td>\n","      <td>0.246200</td>\n","    </tr>\n","    <tr>\n","      <td>662</td>\n","      <td>0.213400</td>\n","    </tr>\n","    <tr>\n","      <td>663</td>\n","      <td>0.175400</td>\n","    </tr>\n","    <tr>\n","      <td>664</td>\n","      <td>0.116600</td>\n","    </tr>\n","    <tr>\n","      <td>665</td>\n","      <td>0.196600</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>0.155400</td>\n","    </tr>\n","    <tr>\n","      <td>667</td>\n","      <td>0.230200</td>\n","    </tr>\n","    <tr>\n","      <td>668</td>\n","      <td>0.226000</td>\n","    </tr>\n","    <tr>\n","      <td>669</td>\n","      <td>0.119000</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>0.192200</td>\n","    </tr>\n","    <tr>\n","      <td>671</td>\n","      <td>0.184200</td>\n","    </tr>\n","    <tr>\n","      <td>672</td>\n","      <td>0.282300</td>\n","    </tr>\n","    <tr>\n","      <td>673</td>\n","      <td>0.182300</td>\n","    </tr>\n","    <tr>\n","      <td>674</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>0.245800</td>\n","    </tr>\n","    <tr>\n","      <td>676</td>\n","      <td>0.153700</td>\n","    </tr>\n","    <tr>\n","      <td>677</td>\n","      <td>0.192300</td>\n","    </tr>\n","    <tr>\n","      <td>678</td>\n","      <td>0.182500</td>\n","    </tr>\n","    <tr>\n","      <td>679</td>\n","      <td>0.200200</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.303200</td>\n","    </tr>\n","    <tr>\n","      <td>681</td>\n","      <td>0.240700</td>\n","    </tr>\n","    <tr>\n","      <td>682</td>\n","      <td>0.310300</td>\n","    </tr>\n","    <tr>\n","      <td>683</td>\n","      <td>0.159400</td>\n","    </tr>\n","    <tr>\n","      <td>684</td>\n","      <td>0.218900</td>\n","    </tr>\n","    <tr>\n","      <td>685</td>\n","      <td>0.133600</td>\n","    </tr>\n","    <tr>\n","      <td>686</td>\n","      <td>0.281500</td>\n","    </tr>\n","    <tr>\n","      <td>687</td>\n","      <td>0.239800</td>\n","    </tr>\n","    <tr>\n","      <td>688</td>\n","      <td>0.215700</td>\n","    </tr>\n","    <tr>\n","      <td>689</td>\n","      <td>0.234500</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.101000</td>\n","    </tr>\n","    <tr>\n","      <td>691</td>\n","      <td>0.218600</td>\n","    </tr>\n","    <tr>\n","      <td>692</td>\n","      <td>0.199300</td>\n","    </tr>\n","    <tr>\n","      <td>693</td>\n","      <td>0.168900</td>\n","    </tr>\n","    <tr>\n","      <td>694</td>\n","      <td>0.234600</td>\n","    </tr>\n","    <tr>\n","      <td>695</td>\n","      <td>0.302500</td>\n","    </tr>\n","    <tr>\n","      <td>696</td>\n","      <td>0.270500</td>\n","    </tr>\n","    <tr>\n","      <td>697</td>\n","      <td>0.207300</td>\n","    </tr>\n","    <tr>\n","      <td>698</td>\n","      <td>0.216300</td>\n","    </tr>\n","    <tr>\n","      <td>699</td>\n","      <td>0.186000</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.213300</td>\n","    </tr>\n","    <tr>\n","      <td>701</td>\n","      <td>0.181100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                             \u001b[38;5;66;03m# Data collators are objects that will form a batch by using a list of dataset elements as input.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2768\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2768\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2771\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2791\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2790\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1082\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1081\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:160\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1060\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m-> 1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1071\u001b[0m         hidden_states,\n\u001b[1;32m   1072\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1077\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:451\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    454\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    455\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:230\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    227\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 230\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:798\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:694\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    691\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    693\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m--> 694\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    697\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:256\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    255\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 256\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:577\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/functional.py:1041\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         lib\u001b[38;5;241m.\u001b[39mcdequantize_blockwise_fp16_fp4(get_ptr(\u001b[38;5;28;01mNone\u001b[39;00m), get_ptr(A), get_ptr(absmax), get_ptr(out), ct\u001b[38;5;241m.\u001b[39mc_int(quant_state\u001b[38;5;241m.\u001b[39mblocksize), ct\u001b[38;5;241m.\u001b[39mc_int(n))\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1041\u001b[0m         lib\u001b[38;5;241m.\u001b[39mcdequantize_blockwise_fp16_nf4(get_ptr(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m, get_ptr(absmax), get_ptr(out), ct\u001b[38;5;241m.\u001b[39mc_int(quant_state\u001b[38;5;241m.\u001b[39mblocksize), ct\u001b[38;5;241m.\u001b[39mc_int(n))\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m quant_state\u001b[38;5;241m.\u001b[39mquant_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/functional.py:412\u001b[0m, in \u001b[0;36mget_ptr\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_void_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["training_arguments = transformers.TrainingArguments(\n","    output_dir=\"results\",\n","    per_device_train_batch_size=1,          # The batch size per GPU/TPU core/CPU for training.\n","    gradient_accumulation_steps=4,          # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n","    optim=\"paged_adamw_8bit\",\n","    save_total_limit=3,                     # If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n","    logging_steps=1,\n","    learning_rate=1e-4,\n","    fp16=True,                              # Beacuse Computation was set to fp16\n","    max_steps=1500,\n","    warmup_ratio=0.05,                      # Proportion of training steps for warm up\n","    lr_scheduler_type='cosine'              # Defines how the learning rate changes while training\n",")\n","#\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=dataset,\n","    args=training_arguments,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n","                                            # Data collators are objects that will form a batch by using a list of dataset elements as input.\n",")\n","model.config.use_cache=False\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-08T07:25:12.569072Z","iopub.status.idle":"2024-02-08T07:25:12.569599Z","shell.execute_reply":"2024-02-08T07:25:12.569354Z","shell.execute_reply.started":"2024-02-08T07:25:12.569329Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\n","\n","generation_config = model.generation_config\n","generation_config.max_new_tokens = 200\n","generation_config.temperature = 0.7\n","generation_config.top_p=0.7\n","generation_config.num_return_sequences=1\n","generation_config.pad_token_id=tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id\n","     \n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T07:25:21.732672Z","iopub.status.busy":"2024-02-08T07:25:21.731803Z","iopub.status.idle":"2024-02-08T07:25:21.739751Z","shell.execute_reply":"2024-02-08T07:25:21.738715Z","shell.execute_reply.started":"2024-02-08T07:25:21.732611Z"},"trusted":true},"outputs":[],"source":["streamer = TextStreamer(\n","    tokenizer, skip_prompt=True, skip_special_tokens=True, use_multiprocessing=False\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T07:25:25.839230Z","iopub.status.busy":"2024-02-08T07:25:25.838848Z","iopub.status.idle":"2024-02-08T07:25:25.848475Z","shell.execute_reply":"2024-02-08T07:25:25.846202Z","shell.execute_reply.started":"2024-02-08T07:25:25.839200Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]}],"source":["pipe=pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    repetition_penalty=1.15,\n","    generation_config=generation_config,\n","    streamer = streamer,\n","    do_sample=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T07:32:37.991758Z","iopub.status.busy":"2024-02-08T07:32:37.991065Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tech-Talk ### Instruction: You are an college chatbot named Helpie. Answer user queries and be respectful.\n","If you don't know any answer just say you don't know.\n","### Human: what is your thoughts about BERT model. Is it better than you ?\n","### Assistant: I think so, because of its ability to generate more diverse and relevant results.. But, in "]}],"source":["output=pipe('''\n","### Instruction: You are an college chatbot named Helpie. Answer user queries and be respectful.\n","If you don't know any answer just say you don't know.\n","### Human: what is your thoughts about BERT model. Is it better than you ?\n","### Assistant:\n","  '''.strip())\n","response=output[0]['generated_text']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-08T07:25:12.577167Z","iopub.status.idle":"2024-02-08T07:25:12.577491Z","shell.execute_reply":"2024-02-08T07:25:12.577345Z","shell.execute_reply.started":"2024-02-08T07:25:12.577330Z"},"trusted":true},"outputs":[],"source":["transformers-cli login\n"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-02-08T07:28:19.637717Z","iopub.status.busy":"2024-02-08T07:28:19.636798Z","iopub.status.idle":"2024-02-08T07:28:20.054192Z","shell.execute_reply":"2024-02-08T07:28:20.053069Z","shell.execute_reply.started":"2024-02-08T07:28:19.637679Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('/kaggle/working/outputs/tokenizer_config.json',\n"," '/kaggle/working/outputs/special_tokens_map.json',\n"," '/kaggle/working/outputs/tokenizer.model',\n"," '/kaggle/working/outputs/added_tokens.json',\n"," '/kaggle/working/outputs/tokenizer.json')"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"/kaggle/working/outputs\")\n","tokenizer.save_pretrained(\"/kaggle/working/outputs\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-08T07:25:12.580729Z","iopub.status.idle":"2024-02-08T07:25:12.581054Z","shell.execute_reply":"2024-02-08T07:25:12.580906Z","shell.execute_reply.started":"2024-02-08T07:25:12.580893Z"},"trusted":true},"outputs":[],"source":["model.push_to_hub(\n","    \"chatbot-model\", use_auth_token=True,create_pr=1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-08T07:25:12.582426Z","iopub.status.idle":"2024-02-08T07:25:12.582900Z","shell.execute_reply":"2024-02-08T07:25:12.582680Z","shell.execute_reply.started":"2024-02-08T07:25:12.582659Z"},"trusted":true},"outputs":[],"source":["model.push_to_hub(\n","    repo_name=\"chatbot-model\",\n","    use_auth_token=True,\n","    commit_message=\"Initial commit\",\n","    organization=\"your_organization\",\n","    private=False\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-08T07:25:12.584420Z","iopub.status.idle":"2024-02-08T07:25:12.584790Z","shell.execute_reply":"2024-02-08T07:25:12.584601Z","shell.execute_reply.started":"2024-02-08T07:25:12.584587Z"},"trusted":true},"outputs":[],"source":["import zipfile\n","\n","# Zip the model files\n","with zipfile.ZipFile('model_files.zip', 'w') as zipf:\n","    zipf.write('/kaggle/working/results')\n","    zipf.write('/kaggle/working/outputs')\n","    zipf.write('/kaggle/working/wandb')\n","    # Add all necessary files\n","\n","# Move the zip file to the /kaggle/working directory\n","import shutil\n","shutil.move('model_files.zip', '/kaggle/working/model_files.zip')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-08T07:25:12.585752Z","iopub.status.idle":"2024-02-08T07:25:12.586076Z","shell.execute_reply":"2024-02-08T07:25:12.585929Z","shell.execute_reply.started":"2024-02-08T07:25:12.585915Z"},"trusted":true},"outputs":[],"source":["import shutil\n","import os\n","\n","# Specify the folder you want to zip\n","folder_to_zip = '/kaggle/working/'\n","\n","# Specify the name for the zip file\n","zip_filename = 'your_folder_name.zip'\n","\n","# Create a zip archive of the entire folder\n","shutil.make_archive(zip_filename, 'zip', folder_to_zip)\n","\n","# Move the zip file to the /kaggle/working directory\n","shutil.move(zip_filename + '.zip', '/kaggle/working/' + zip_filename + '.zip')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-08T07:25:12.587170Z","iopub.status.idle":"2024-02-08T07:25:12.587495Z","shell.execute_reply":"2024-02-08T07:25:12.587348Z","shell.execute_reply.started":"2024-02-08T07:25:12.587335Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM\n","\n","# Load the saved model\n","loaded_model = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/outputs\")\n","\n","# Now, 'loaded_model' is ready for inference or further fine-tuning.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-08T07:25:12.589151Z","iopub.status.idle":"2024-02-08T07:25:12.589465Z","shell.execute_reply":"2024-02-08T07:25:12.589325Z","shell.execute_reply.started":"2024-02-08T07:25:12.589311Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained(\"/kaggle/working/outputs\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4403248,"sourceId":7562136,"sourceType":"datasetVersion"},{"datasetId":4413226,"sourceId":7581329,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
